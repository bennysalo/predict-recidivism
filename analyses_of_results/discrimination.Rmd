---
title: "Discrimination of trained models"
date: "`r Sys.Date()`"
output: github_document
---

## Setup

```{r message=FALSE, warning=FALSE}
devtools::load_all(".")
library(dplyr)
library(ggplot2)
```


# Training set

Available in the /data folder of the package is the data frame `model_AUCs_training_set`. This is created by the function caret::resamples and can be used with caret functions to display the discrimination numbers in the 10 repeated 10-fold cross-validations. For example, we can make a box plot of all 100 AUV values for the 24 models.

```{r message=FALSE}
library(caret)
bwplot(model_AUCs_training_set, metric = "ROC")
```
Model names can be looked up in the model_grid

```{r}
model_desc <- model_grid[c("model_name", "outcome", 
                           "predictors", "model_type")]
knitr::kable(model_desc)
```

## Discrimination in cross validated training set: `auc_tbl_cv`

The discrimination varies widely over the 100 cross-validation sets for all models. We are interested in the mean and standard errors of these.

We create a table of confidence intervals for the cross-validation results. The standard errors, and thereby the confidence intervals, will be narrower with more folds and is best interpreted as "what is the best cross-validated model in this sample".

Confidence interval is calculated by first transforming AUC-values to logits of the AUC-values. After calculation of mean, and confidence intervals for the logit they are transformed back to AUC-values.

```{r}
# Functions from dplyr used (select, mutate, group_by, summarise)
auc_tbl_cv <-
model_AUCs_training_set$values %>% 
  select(Resample, ends_with(match = "ROC")) %>% 
  tidyr::gather(-Resample, key = model, value = ROC) %>%
  mutate(logit_ROC = log(ROC / (1-ROC))) %>% 
  group_by(model) %>% 
  summarise(n = n(),
            mean_logit  = mean(logit_ROC),
            se_logit    = sqrt(var(logit_ROC) / n),
            ci_lo_logit = mean_logit + qnorm(0.025) * se_logit,
            ci_hi_logit = mean_logit + qnorm(0.975) * se_logit,
            mean_auc_cv    = exp(mean_logit)  / (1 + exp(mean_logit)),
            ci_lo_cv   = exp(ci_lo_logit) / (1 + exp(ci_lo_logit)),
            ci_hi_cv   = exp(ci_hi_logit) / (1 + exp(ci_hi_logit))) %>% 
  select(model, mean_auc_cv, ci_lo_cv, ci_hi_cv) %>% 
  mutate(model = stringr::str_replace(model, pattern = "~ROC", ""))

```
The data frame `auc_tbl_cv` contains the mean AUC with 95% confidence intervals for all models in the model grid. 

## Discrimination in test set: `auc_tbl_ts`

Using the package `pROC` we can calculate the AUC with confidence intervals. For this we need the observed outcomes and the predicted probabilities. The predicted probabilities are included in the package in the data frame `test_set_predictions` (in the /data folder).

We use bootstrap to calculate the confidence intervals which means this chunk takes a few minutes to run.
```{r}
# Get the names of predictions of violent and generalised recidivism seperately.
model_names_G <- stringr::str_subset(names(test_set_predictions), 
                                     pattern = "gen_") 
model_names_V <- stringr::str_subset(names(test_set_predictions), 
                                     pattern = "vio_") 

# Do ROC analysis using predictions with the relevant outcome
# First all predictions of general recidivism
# (ts stands for 'test set')
roc_list_ts_G <- 
  purrr::map(.x = test_set_predictions[model_names_G],
             .f = ~ pROC::roc(test_set_predictions$reoffenceThisTerm, .x))

# Then all predictions of violent recidivism
roc_list_ts_V <- 
  purrr::map(.x = test_set_predictions[model_names_V],
             .f = ~ pROC::roc(test_set_predictions$newO_violent, .x))

# Combine the two lists
roc_list_ts   <- c(roc_list_ts_G, roc_list_ts_V)

# Bootstrap confidence intervals for all AUC-values
set.seed(2803)
auc_ci_list   <- 
  purrr::map(roc_list_ts,
             .f = ~ pROC::ci.auc(.x,
             method = "bootstrap",
             progress = "none"))


```
The results are at this point stored in a list (`auc_ci_list`). The following chunk places them in data_frame that we call `auc_tbl_ts`

```{r}
#(_ts stands for "test set")
# Create a function to extract auc and its confidence interval
get_ci <- function(ci.auc_result) {
  data_frame(auc_ts      = ci.auc_result[2], 
             ci_lower_ts = ci.auc_result[1], 
             ci_upper_ts = ci.auc_result[3])
}

# Apply this function to all results in 'the 'auc_tbl_ts'
auc_tbl_ts <- purrr::map_df(auc_ci_list, get_ci)

# Amend the data frame with the names of the models
auc_tbl_ts <- data.frame(model = names(auc_ci_list), auc_tbl_ts,
                         stringsAsFactors = FALSE)
```

## Join tables `auc_tbl_cv` and `auc_tbl_ts`
We join the tables with AUC-values in *training set* and *test set* and then join these to the descriptive columns i model_grid.

```{r}
auc_tbl <- dplyr::full_join(auc_tbl_cv, auc_tbl_ts, by = "model") %>% 
  full_join(model_desc, auc_tbl, by = c("model" = "model_name"))
```

We add to this table figures for Cohen's d calculated from AUC using a formula in Table 1 in Ruscio (2008): $d = \sqrt2 \phi^{-1}CL$ where CL refers to "Common Language Effect Size" which in this case is the same as AUC and $\phi^{-1}$ is the inverse of the normal cumulative distribution function (i.e. the z-score that correspond to a cumulative percentage in a normal distribution). 

This is the formula assuming equal group sizes. This assumption does not hold for violent recidivism but retains the property of AUC of not being affected by base rates and thus makes comparison between violent and general recidivism more straight forward. 

Cohen's d has the advantage over AUC in that it is an effect size that is linear. With that I mean that the difference between d = 0.1 and d = 0.2 is the same as the difference between d = 1.1 and d = 1.2. On the contrary, the difference between AUC = 0.60 and AUC = 0.65 is smaller than the difference between AUC = 0.90 and AUC = 0.95. This makes d more convinient for comparisons between pairs of models.

The function below can be used to transform any AUC value to Cohen's d.

```{r}
calc_d_from_AUC <- function(AUC) {
  sqrt(2) * qnorm(AUC) 
}
```

We add Cohen's d calculated from the mean AUC in the training set and the estimated AUC in the test set.

```{r}
auc_tbl <- auc_tbl %>% 
  mutate(d_cv = calc_d_from_AUC(mean_auc_cv),
         d_ts = calc_d_from_AUC(auc_ts))
```

The `auc_tbl`now contains discimination values for all 36 models. Many of the following analyses focus on the main analyses excluding models with a single RITA-factor as predictor. We therfore create a seperate tables with just main analyses (`auc_tbl_mains`) and a table with just the analyses of RITA-dimensions (`auc_tbl_dims`).

```{r}
# Extract model names of main analyses
model_names_main <-
  model_grid %>%
  filter(analysis == "Main analyses") %>%
  select(model_name) %>% 
  purrr::as_vector(.type = "character")

# Filter auc_tbl
auc_tbl_mains <- filter(auc_tbl, model %in% model_names_main)
auc_tbl_dims  <- filter(auc_tbl, !(model %in% model_names_main))
```

## Discrimination in main analyses

We can compare the average Cohen's d between different model types
```{r}
mean_perf_by_type <-
auc_tbl_mains %>% 
  group_by(model_type) %>% 
  summarise(mean(d_cv), mean(d_ts))

mean_perf_by_type
```
Average difference between elastic net and logistic regression models in the training set is...

```{r}
mean_perf_by_type %>% filter(model_type == "Elastic net") %>%
  select(`mean(d_cv)`) -
  mean_perf_by_type %>% filter(model_type == "Logistic regression") %>%
  select(`mean(d_cv)`)
```


### Figure 1

```{r}
auc_plot <- auc_tbl_mains %>% 
  ggplot(aes(x = auc_ts, y = mean_auc_cv)) + 
  geom_point(aes(shape = predictors), size = 3) +
  geom_errorbar(aes(ymin = ci_lo_cv, ymax = ci_hi_cv)) +
  geom_errorbarh(aes(xmin = ci_lower_ts, xmax = ci_upper_ts)) + 
  scale_shape_manual(values = c(16, 4, 1, 17), 
                     name   = "Predictor set") +
  # scale_y_continuous(limits = c(0.55, 0.87)) +
  geom_abline(lty = 2) +
  facet_grid(outcome ~ model_type) +
  ylab("AUC in cross-validation samples") +
  xlab("AUC in independent test sample") + 

  ggthemes::theme_tufte(base_family = "sans") +
  theme(legend.position = "bottom")

```
```{r fig.width=6.5}
auc_plot
```


## Table 2

Below we filter out the model with the best discrimination in the training set for each outcome - predictor set pair. We select the columns we report in Table 2 of the manuscript.

```{r}
aus_tbl_best_mains <-
  auc_tbl_mains %>% 
  group_by (outcome, predictors) %>% 
  filter(mean_auc_cv == max(mean_auc_cv)) %>% 
  arrange(outcome, mean_auc_cv) 

table2 <- 
  bind_rows(aus_tbl_best_mains, auc_tbl_dims) %>% 
    arrange(outcome) %>% 
    select(outcome, predictors, 
           mean_auc_cv, ci_lo_cv, ci_hi_cv, d_cv,
           auc_ts, ci_lower_ts, ci_upper_ts, d_ts, 
           model_type) %>% 
    mutate_at(.vars = c(3:10), .funs = round, digits = 3) %>% 
    mutate_at(.vars = vars(d_cv, d_ts), .funs = round, digits = 2)


```

Print
```{r}
knitr::kable(table2)
```

```{r eval=FALSE, include=FALSE}
devtools::wd()
write.csv2(table2, "output/table2.csv")
```

# The value of customization

We believe that the good performance of the models is partly due to the customization of weights in the models. To illustrate this we predict a form of recidivism that the models are not trained to predict. That is we let general recidivism be predicted by models that are trained to predict violent recidivism and vice versa.

```{r}
cross_pred_G_by_V <- 
  purrr::map(.x = test_set_predictions[model_names_V],
             .f = ~ pROC::roc(test_set_predictions$reoffenceThisTerm, .x))


cross_pred_V_by_G <- 
  purrr::map(.x = test_set_predictions[model_names_G],
             .f = ~ pROC::roc(test_set_predictions$newO_violent, .x))

cross_pred_list <- c(cross_pred_G_by_V, cross_pred_V_by_G)

# We use the same seed as in the correct analyses 
set.seed(2803)
cross_pred_ci_list <- 
  purrr::map(cross_pred_list,
             .f = ~ pROC::ci.auc(.x,
             method = "bootstrap",
             progress = "none"))


cross_pred_tbl <- 
  purrr::map_df(cross_pred_ci_list, get_ci)

cross_pred_tbl <- 
  data.frame(model = names(cross_pred_ci_list), cross_pred_tbl)


```
### Print sessionInfo
```{r}
sessionInfo()
```

