---
title: "Discrimination of trained models"
author: "Benny Salo"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r}
library(tidyverse)
```


# Training set

The package comes with the data frame `model_AUCs_training_set`. This is created by the function caret::resamples and can be used with caret functions to display the discrimination numbers in the 10 repeated 10-fold cross-validations. For example, we can make a box plot of all 100 AUV values for the 24 models.

```{r message=FALSE}
library(caret)
bwplot(model_AUCs_training_set, metric = "ROC")
```
Model names can be looked up in the model_grid

```{r}
model_desc <- model_grid[c("model_name", "outcome", 
                           "predictors", "model_type")]
model_desc
```

The discrimination varies widely over the 100 cross-validation sets for all models. We are interested in the mean and standard errors of these.

We create a table of confidence intervals for the cross-validation results. The standard errors, and thereby the confidence intervals, will be narrower with more folds and is best interpreted as "what is the best cross-validated model in this sample".

Confidence interval is calculated by first transforming AUC-values to logits of the AUC-values. After calculation of mean, and confidence intervals for the logit they are transformed back to AUC-values.

```{r}
auc_tbl_cv <-
model_AUCs_training_set$values %>% 
  select(Resample, ends_with(match = "ROC")) %>% 
  gather(-Resample, key = model, value = ROC) %>%
  mutate(logit_ROC = log(ROC / (1-ROC))) %>% 
  group_by(model) %>% 
  summarise(n = n(),
            mean_logit  = mean(logit_ROC),
            se_logit    = sqrt(var(logit_ROC) / n),
            ci_lo_logit = mean_logit + qnorm(0.025) * se_logit,
            ci_hi_logit = mean_logit + qnorm(0.975) * se_logit,
            mean_auc_cv    = exp(mean_logit)  / (1 + exp(mean_logit)),
            ci_lo_cv   = exp(ci_lo_logit) / (1 + exp(ci_lo_logit)),
            ci_hi_cv   = exp(ci_hi_logit) / (1 + exp(ci_hi_logit))) %>% 
  select(model, mean_auc_cv, ci_lo_cv, ci_hi_cv) %>% 
  mutate(model = as.factor(str_replace(model, pattern = "~ROC", "")))

```
The data frame `auc_tbl_cv` contains the mean AUC with 95% confidence intervals for all 24 models.

# Test set

Using the package `pROC` we can calculate the AUC with confidence intervals. For this we need the observed outcomes and the predicted probabilities. The predicted probabilities are included in the package in the data frame `test_set_predictions`.

We use bootstrap to calculate the confidence intervals which means this chunk takes a few minutes to run.
```{r}
library(pROC)
# Get the names of predictions of violent and generalised recidivism seperately.
model_names_G <- str_subset(names(test_set_predictions), pattern = "gen_") 
model_names_V <- str_subset(names(test_set_predictions), pattern = "vio_") 

# Do ROC analysis using predictions with the relevant outcome
# First all predictions of general recidivism
# (ts stands for 'test set')
roc_list_ts_G <- map(.x = test_set_predictions[model_names_G],
                   .f = ~roc(test_set$reoffenceThisTerm, .x))

# Then all predictions of violent recidivism
roc_list_ts_V <- map(.x = test_set_predictions[model_names_V],
                   .f = ~roc(test_set$newO_violent, .x))
# Combine the two lists
roc_list_ts   <- c(roc_list_ts_G, roc_list_ts_V)

# Bootstrap confidence intervals for all AUC-values
set.seed(2803)
auc_ci_list   <- map(roc_list_ts, 
                     .f = ~pROC::ci.auc(.x, 
                                        method = "bootstrap", 
                                        progress = "text"))

```
The results are at this point stored in a list (`auc_ci_list`). The following chunk places them in data_frame that we call `auc_tbl_ts`

```{r}
#(_ts stands for "test set")
# Create a function to extract auc and its confidence interval
get_ci <- function(ci.auc_result) {
  data_frame(auc_ts      = ci.auc_result[2], 
             ci_lower_ts = ci.auc_result[1], 
             ci_upper_ts = ci.auc_result[3])
}

# Apply this function to all results in 'the 'auc_tbl_ts'
auc_tbl_ts <- map_df(auc_ci_list, get_ci)

# Amend the data frame with the names of the models
auc_tbl_ts <- data.frame(model = names(auc_ci_list), auc_tbl_ts)
```
We join the tables with AUC-values in training and test set and then join these to the descriptive columns i model_grid.

```{r message=FALSE}
auc_tbl <- full_join(auc_tbl_cv, auc_tbl_ts, by = "model") %>% 
  full_join(model_desc, auc_tbl, by = c("model" = "model_name"))
```

We add to this table figures for Cohen's d calculated from AUC using a formula in Table 1 in Ruscio (2008): $d = \sqrt2 \phi^{-1}CL$ where CL refers to "Common Language Effect Size" which in this case is the same as AUC and $\phi^{-1}$ is the inverse of the normal cumulative distribution function (i.e. the z-score that correspond to a cumulative percentage in a normal distribution). 

This is the formula assuming equal group sizes. This assumption does not hold for violent recidivism but retains the property of AUC of not being affected by base rates and thus makes comparison between violent and general recidivism more straight forward. 

Cohen's d has the advantage over AUC in that it is an effect size that is linear. With that I mean that the difference between d = 0.1 and d = 0.2 is the same as the difference between d = 1.1 and d = 1.2. On the contrary, the difference between AUC = 0.60 and AUC = 0.65 is smaller than the difference between AUC = 0.90 and AUC = 0.95. This makes d more convinient for comparisons between pairs of models.

The function below can be used to transform any AUC value to Cohen's d.

```{r}
calc_AUC_from_d <- function(AUC) {
  sqrt(2) * qnorm(AUC) 
}
```

We add Cohen's d calculated from the mean AUC in the training set and the estimated AUC in the test set.

```{r}
auc_tbl <- auc_tbl %>% 
  mutate(d_cv = calc_AUC_from_d(mean_auc_cv),
         d_ts = calc_AUC_from_d(auc_ts))
```



# Table 2

Below we filter out the model with the best discrimination in the training set for each outcome - predictor set pair. We select the columns we report in Table 2 of the manuscript.

```{r}
best_models_tbl <-
  auc_tbl %>% 
  group_by (outcome, predictors) %>% 
  filter(mean_auc_cv == max(mean_auc_cv)) %>% 
  arrange(outcome, mean_auc_cv) %>% 
  select(outcome, predictors, 
         mean_auc_cv, ci_lo_cv, ci_hi_cv, d_cv,
         auc_ts, ci_lower_ts, ci_upper_ts, d_ts, 
         model_type) %>% 
  mutate_at(.vars = c(1:8), .funs = round, digits = 3) %>% 
  mutate_at(.vars = vars(d_cv, d_ts), .funs = round, digits = 2)

write.csv2(best_cv_tbl, "best_cv_tbl.csv")
```
