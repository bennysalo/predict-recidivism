---
title: "Figure of model performances"
output: html_notebook
date: 2017-11-10
---
```{r}
library(caret)
library(tidyverse)
library(stringr)
```

Load grid data frames - contains model information
```{r}
glm_grid    <- readRDS(file = "glm_grid.rds")
glmnet_grid <- readRDS(file = "glmnet_grid.rds")
rf_grid     <- readRDS(file = "rf_grid.rds")
```



# Load the lists of models
```{r}
train_mods_glm <- readRDS("train_mods_glm.rds")
names(train_mods_glm) <- glm_grid$model_name
train_mods_glmnet <- readRDS("train_mods_glmnet")
names(train_mods_glmnet) <- glmnet_grid$model_name
train_mods_rf <- readRDS("train_mods_rf")
names(train_mods_rf) <- rf_grid$model_name

```



Combine the two lists to one and pass it to 'caret:resamples'
```{r}
model_list        <- c(train_mods_glm, train_mods_glmnet, train_mods_rf)
model_comparisons <- resamples(model_list)
```

Screen the results with boxplots
```{r}
bwplot(model_comparisons, metric = "ROC")
```

Create a table of confidence intervals for the cross-validation results. These will be narrower with more folds and is best interpreted as "what is the best cross-validated model in this sample".

Confidence interval is calculated by first transformin AUC-values to logits of the AUC-values. After calculation of mean, and confidence intervals for the logit they are transformed back to AUC-values.

```{r}
auc_tbl_cv <-
model_comparisons$values %>% 
  select(Resample, ends_with(match = "ROC")) %>% 
  gather(-Resample, key = model, value = ROC) %>%
  mutate(logit_ROC = log(ROC / (1-ROC))) %>% 
  group_by(model) %>% 
  summarise(n = n(),
            mean_logit  = mean(logit_ROC),
            se_logit    = sqrt(var(logit_ROC) / n),
            ci_lo_logit = mean_logit + qnorm(0.025) * se_logit,
            ci_hi_logit = mean_logit + qnorm(0.975) * se_logit,
            mean_auc_cv    = exp(mean_logit)  / (1 + exp(mean_logit)),
            ci_lo_cv   = exp(ci_lo_logit) / (1 + exp(ci_lo_logit)),
            ci_hi_cv   = exp(ci_hi_logit) / (1 + exp(ci_hi_logit))) %>% 
  select(model, mean_auc_cv, ci_lo_cv, ci_hi_cv) %>% 
  mutate(model = as.factor(str_replace(model, pattern = "~ROC", "")))

```

To estimate the generalizability to other samples we predict scores on the test set and calcualte ROC scores there.

Predict scores in test_set
```{r}

# Create a function to extract predictions for a given model
predict_test_set<- function(model) {
  predict.train(model, newdata = test_set, type = "prob")[[2]]
}

# Apply this function to all models in the list
predictions <- map_df(model_list, .f = ~ predict_test_set(.x))

saveRDS(predictions, "predictions.rds")
```


Calculate AUCs and confidence intervals
```{r}
require(pROC)
# Get the names of predictions of violent and generalised recidivism seperately.
model_names_V <- str_subset(names(predictions), pattern = "V_") 
model_names_G <- str_subset(names(predictions), pattern = "G_") 

# Do ROC analysis using predictions with the relevant outcome
# First all predictions of violent recidivism
# (ts stands for 'test set')
roc_list_ts_V <- map(.x = predictions[model_names_V],
                   .f = ~roc(test_set$newO_violent, .x))

# Then all predictions of general recidivism
roc_list_ts_G <- map(.x = predictions[model_names_G],
                   .f = ~roc(test_set$reoffenceThisTerm, .x))

# Combine the two lists
roc_list_ts   <- c(roc_list_ts_V, roc_list_ts_G)

# Bootstrap confidence intervals for all AUC-values
set.seed(2803)
auc_ci_list   <- map(roc_list_ts, .f = ~ci.auc(.x, method = "bootstrap"))

```

```{r}
#(_ts stands for "test set")
# Create a function to extract auc and its confidence interval
get_ci <- function(ci.auc_result) {
  data_frame(auc_ts      = ci.auc_result[2], 
             ci_lower_ts = ci.auc_result[1], 
             ci_upper_ts = ci.auc_result[3])
}

# Apply this function to all results in 'the 'auc_tbl_ts'
auc_tbl_ts <- map_df(auc_ci_list, get_ci)

# Amend the data frame with the names of the models
auc_tbl_ts <- data.frame(model = names(auc_ci_list), auc_tbl_ts)
```

Join the results of training set analyses and test set analyses

```{r}
auc_tbl <- full_join(auc_tbl_cv, auc_tbl_ts, by = "model") %>% 
  mutate(newO_type = ifelse(str_detect(model, pattern = "V_"), 
                            "Violent recidivism", 
                            "General recidivism"),
         
         model_type = ifelse(str_detect(model, pattern = "rf"), "Random forest", 
                      ifelse(str_detect(model, pattern = "glmnet"), 
                             "Elastic net","Logistic regression")),
         
         Predictors = ifelse(str_detect(model, "dynamic"), "Dynamic items",
                       ifelse(str_detect(model, "all-bgn"), 
                              "Static and dynamic items",
                       ifelse(str_detect(model, "incl-te"), "All predictors",
                              "Static items"))))
   

auc_tbl$Predictors <- factor(auc_tbl$Predictors, levels = 
                                 c("All predictors", "Static and dynamic items", 
                                   "Static items",  "Dynamic items"))     

auc_tbl$model_type <- factor(auc_tbl$model_type, levels = 
                                 c("Logistic regression", "Elastic net", "Random forest"))   

```

Add column for predictors

```{r}
# model_patterns <- c("dynamic", 
#                     "staticp", 
#                     "all-bgn", 
#                     "incl-te")
# matched_preds  <- c("Rita-Items", 
#                     "Static", 
#                     "Static and RITA-Items", 
#                     "All incl. term variables")

auc_tbl <- auc_tbl %>% 
  mutate(predictors = 
           ifelse(str_detect(model, "dynamic"), "Rita-Items",
           ifelse(str_detect(model, "staticp"), "Static",
           ifelse(str_detect(model, "all-bgn"), "Static and RITA-Items",
                  "All incl. term variables"))))



```



```{r}
saveRDS(auc_tbl, "auc_tbl.rds")
```


```{r}
best_models_tbl <-
  auc_tbl %>% 
  group_by (newO_type, predictors) %>% 
  filter(mean_auc_cv == max(mean_auc_cv)) %>% 
  arrange(newO_type, mean_auc_cv)

best_cv_tbl <- best_models_tbl %>% 
  select(newO_type, predictors, 
         mean_auc_cv, ci_lo_cv, ci_hi_cv, 
         auc_ts, ci_lower_ts, ci_upper_ts) %>% 
  mutate_at(.vars = c(1:6), .funs = round, digits = 3)

write.csv2(best_cv_tbl, "best_cv_tbl.csv")



```

