---
title: "Training models"
output: github_document
date: 2017-11-07
---

# Setup

Load packages

```{r}
library(caret)
library(tidyverse)
library(pROC)
library(stringr)
library(lavaan)
```

# Load data. 
Exclusion criteria are descibed in data-raw/analyzed_data.md. We are not authorized to make raw data publicly available and the data loaded here is unfortunately not publicly accessible.

```{r message=FALSE, warning=FALSE}
rm(list = ls())
setwd("..")
analyzed_data <- 
  readRDS("data-raw/analyzed_data.rds")
```
These are the 15 categories for the current offence. They are included in the static predictors.

```{r}

(offence_variables <- 
  stringr::str_subset(names(analyzed_data), "^o_"))

# Static predictors as used in training
static_preds <- 
   c("ageFirstSentence_mr", "ageFirst_missing", "ageAtRelease", 
     "ps_escapeHistory", "ps_prisonTerms_mr", "ps_comServiceTerms_mr", 
     "ps_remandTerms_mr", "ps_defaultTerms_mr", "ps_info_missing", 
     offence_variables)

# Dynamic predictors
(rita_items <- 
  stringr::str_subset(names(analyzed_data), "^i_"))

static_n_rita <- c(static_preds, rita_items)

incl_term <- c(static_preds, rita_items, 
               "openPrison", "conditionalReleaseOutcome", 
               "crimeDuringSentence", "supervisedParole")

# Combined vector of all variables: outcomes and predictors
all_variables  <- c(incl_term, "reoffenceThisTerm", "newO_violent")

```

# Creating subsets
(it would be possible to make sure matched pairs are placed in the same fold - we chose not to do this.)

First devide data into a training set and a test set in ratio 3 to 1

```{r}
set.seed(3010)
four_folds <-createFolds(y = analyzed_data$reoffenceThisTerm, k = 4)

test_set     <- analyzed_data[ (1:nrow(analyzed_data) %in% 
                                   four_folds[[4]]), all_variables]
training_set <- analyzed_data[!(1:nrow(analyzed_data) %in% 
                                   four_folds[[4]]), all_variables]

nrow(test_set)
nrow(training_set)


```

Then make 10 sets of 10 folds 
```{r}
# ten_folds_training  <- createFolds(y = training_set$reoffenceThisTerm, k = 10)
# twenty_by_ten_folds <- createMultiFolds(y = training_set$reoffenceThisTerm, k = 10, times = 20)
ten_by_ten_folds <- createMultiFolds(y = training_set$reoffenceThisTerm, k = 10, times = 10)
```


Now set up the cross validation schemes using these folds. 


```{r}

myControl <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 10,
  summaryFunction = twoClassSummary,
  classProbs = TRUE, 
  verboseIter = TRUE,
  savePredictions = "final",
  index = ten_by_ten_folds)
```






# Create a data frame of all the combinations for the models


```{r}
model_type <- c("glm", "glmnet", "rf")
outcomes   <- c("reoffenceThisTerm", "newO_violent")
predictors <- c("static_preds", "rita_items","static_n_rita", "incl_term")
model_grid <- expand.grid(model_type, outcomes, predictors, 
                         stringsAsFactors = FALSE)
colnames(model_grid) <- c("model_type", "outcome", "predictors")
```

Create a function for writing formulas used in `caret::train`
```{r}
write_formula <- function(outcome, predictors) {
  rhs            <- eval(parse(text = model_grid$predictors))
  tested_formula <- as.formula(paste(outcome, " ~ ", 
                                     paste(rhs, collapse = " + ")))
  return(tested_formula)
}
```



```{r}
model_grid$tested_formula <-
  purrr::map2(
    .x = model_grid$outcome,
    .y = model_grid$predictors,
    .f = ~ write_formula(.x, .y
    )
  )
  
```


```{r}
model_grid$n_preds <-
  purrr::map_int(
    .x = model_grid$predictors,
    .f = ~ length(eval(parse(text = .x)))
    )
  
  
```


Add list of arguments to use with `caret::train`
```{r}
write_arg_list <- function(model_type, outcome, predictors) {
  predictors <- eval(parse(text = predictors))
  if (model_type == "glm") {
    specific_args <- list(x         = training_set[predictors],
                          y         = training_set[[outcome]],
                          method = "glm",
                          family = "binomial",
                          metric    = "ROC",
                          trControl = myControl
                          )
    # We seem to need to use the formula method of caret::train
    # Might be because of the factors and ordinal predictors?
  } else if (model_type == "glmnet") {
    specific_args <- list(form      = as.formula(
                            paste(outcome, " ~ ",
                                  paste(predictors, collapse = " + "))),
                          data      = training_set,
                          method    = "glmnet",
                          family    = "binomial",
                          standardize = TRUE,
                          metric    = "ROC",
                          trControl = myControl,
                          tuneGrid = expand.grid(
                            alpha  = c(0, 0.2, 0.4, 0.6, 0.8, 1),
                            lambda = seq(0, 3, by = 0.02))
                          )
  } else if (model_type == "rf") {
    # m_try sequency depending on number of predictors
    # includes sqrt() (i.e. 4/8), other powers are 1/8 increments
    n_preds <- length(predictors)
    m_try_seq <- round(n_preds^((1:7)/8))
    specific_args <- list(x         = training_set[predictors],
                          y         = training_set[[outcome]],
                          method    = "rf",
                          metric    = "ROC",
                          trControl = myControl,
                          tuneGrid  = expand.grid(mtry = m_try_seq)
                          )
  }
  
return(specific_args)
}


model_grid$specific_args <-
  purrr::pmap(
    .l = list(
      model_type = model_grid$model_type,
      outcome    = model_grid$outcome,
      predictors = model_grid$predictors
      ),
    .f = ~ write_arg_list(..1, ..2, ..3)
  )
```

Split into subsets of models for easier management
```{r}
glm_grid    <- model_grid %>% filter(model_type == "glm")
glmnet_grid <- model_grid %>% filter(model_type == "glmnet")
rf_grid     <- model_grid %>% filter(model_type == "rf")
```
Strip data from the train output.

function
```{r}
strip_data <- function(train_output) {
  train_output$call <- NA
  train_output$trainingData <- NA
  train_output
}
```



```{r}



# glm ---------------
start <- Sys.time()
train_mods_glm    <- map(.x = glm_grid$specific_args, 
                        .f = ~do.call(caret::train, .x))
Sys.time() - start

train_mods_glm <- map(train_mods_glm, strip_data)
saveRDS(train_mods_glm, file = "train_mods_glm")
rm(train_mods_glm)

# glmnet -------------

start <- Sys.time()
train_mods_glmnet <- map(.x = glmnet_grid$specific_args,
                        .f = ~do.call(caret::train, .x))
Sys.time() - start

train_mods_glmnet <- map(train_mods_glm, strip_data)
saveRDS(train_mods_glmnet, file = "train_mods_glm")
rm(train_mods_glmnet)

#rf ------------------

start <- Sys.time()
train_mods_rf     <- map(.x = rf_grid$specific_args, 
                        .f = ~do.call(caret::train, .x))
Sys.time() - start

train_mods_rf <- map(train_mods_glm, strip_data)
saveRDS(train_mods_rf, file = "train_mods_glm")
rm(train_mods_rf)


```

