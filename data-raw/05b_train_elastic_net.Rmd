---
title: "Training elastic net models"
author: "Benny Salo"
date: "2018-04-13"
output: github_document
---

Clear environment. Load previous results from the package.
```{r}
rm(list = ls())
devtools::load_all(".")
```

Training set is defined in 01_analyzed_data.Rmd
```{r message=FALSE, warning=FALSE}
devtools::wd()
training_set <- readRDS("not_public/training_set.rds")
```

Get the part of model_grid that are elastic net models.
```{r}
library(dplyr)
glmnet_grid <- model_grid %>% filter(
  model_type == "Elastic net")
```

We seem to need to use the formula method of caret::train for glmnet. (Might be because of the factors and ordinal predictors?)

We introduce a new column where we write the formula.

```{r}
write_formula <- function(lhs, rhs) {
  as.formula(paste(lhs, "~",  paste(rhs, collapse = " + ")))
}

glmnet_grid$formula <- 
  purrr::map2(.x = glmnet_grid$lhs, 
              .y = glmnet_grid$rhs,
              .f = ~write_formula(.x, .y))


```
Checks. (Could be moved to a test file)

```{r}
# All entries in glmnet_grid$formula should be formulas
all(purrr::map(glmnet_grid$formula, class) == "formula")

# All formulas should include the corresponding outcome
outcome_in_formula <-
  purrr::map2_lgl(
    .x = as.character(glmnet_grid$formula),
    .y = glmnet_grid$lhs,
    .f = ~ stringr::str_detect(string = .x, pattern = .y)
    )
all(outcome_in_formula)

# The number of plusses in the formula should equal 
# the number of predictors - 1

n_plusses <- purrr::map2_int(
  .x = as.character(glmnet_grid$formula),
  .y = glmnet_grid$lhs,
  .f = ~ stringr::str_count(string = .x, pattern = "\\+")
  ) 

n_preds <- purrr::map_dbl(.x = glmnet_grid$rhs,
                      .f = ~ length(.x) - 1)
                      

all(n_plusses == n_preds)
```

We are also going to want to adjust the tested values for parameter alpha. We create a new column for this. The tested values in the first run will be 
0, .2, .4, .6, .8, and 1.
```{r}
glmnet_grid$alpha <- vector("list", nrow(glmnet_grid))
glmnet_grid$alpha <- purrr::map(.x = glmnet_grid$alpha, 
                                .f = ~c(0, 0.2, 0.4, 0.6, 0.8, 1))


```
Checks
```{r}
length(glmnet_grid$alpha) == 8
all(purrr::map(glmnet_grid$alpha, length) == 6)
all(purrr::map(glmnet_grid$alpha, class) == "numeric")
```


Train each model (rows) in the grid according to specifications in the grid. Place results in the train_result columns (previously intitiated).

The values of the following two columns are varied: formula and alpha.
Predictors are standarized before training to make the penalty work the same way for all predictors. A sequence between 0 and 3 is tested for tuning parameter lambda.

(Record how long it takes to run.)
```{r}

start <- Sys.time()

trained_mods_glmnet <- 
  purrr::map2(
    .x = glmnet_grid$formula,
    .y = glmnet_grid$alpha,
    .f = ~ caret::train(
      form      = .x,
      data      = training_set,
      method    = "glmnet",
      family    = "binomial", # passed to glmnet, define as logistic regression
      standardize = TRUE,     # passed to glmnet, explicitly standardize  
      metric    = "ROC",
      trControl = ctrl_fun_training,
      tuneGrid  = expand.grid(alpha  = .y,
                              lambda = seq(0, 3, by = 0.02))
      )
    )
    

time_to_run <- Sys.time() - start
time_to_run

```
Name models

```{r}
names(trained_mods_glmnet) <- glmnet_grid$model_name
```


```{r message=FALSE, warning=FALSE}
devtools::wd()
saveRDS(trained_mods_glmnet, "not_public/trained_mods_glmnet.rds")
```

# Update models

Create a column with updated alpha-values
```{r}
glmnet_grid$alpha2 <- vector("list", nrow(glmnet_grid))
```

We narrow in our search for the best mixing parameter (alpha) based on the first search.

```{r}
plot_comparison <- function(model_list, nr, upper_xlim = 0.3, 
                           lower_ylim_range = 0.005) {
  model <- model_list[[nr]]
  best_ROC <- max(model$results$ROC)
  caret::plot.train(model, 
                    xlim = c(0, upper_xlim),
                    ylim = c(best_ROC - lower_ylim_range, best_ROC),
                    main = names(model_list)[[nr]])
}
```


```{r}
plot_comparison(trained_mods_glmnet, 1)
glmnet_grid$alpha2[[1]] <- c(0.5, 0.6, 0.7, 0.8, 0.9, 1.0)
```
```{r}
plot_comparison(trained_mods_glmnet, 2)
glmnet_grid$alpha2[[2]] <- c(0, 0.1, 0.2)
```
```{r}
plot_comparison(trained_mods_glmnet, 3, upper_xlim = 1)
glmnet_grid$alpha2[[3]]  <- c(0.1, 0.2, 0.3)
```

```{r}
plot_comparison(trained_mods_glmnet, 4, upper_xlim = 0.8, lower_ylim_range = 0.03)
glmnet_grid$alpha2[[4]]  <- c(0, 0.1)
```
```{r}
plot_comparison(trained_mods_glmnet, 5)
glmnet_grid$alpha2[[5]]  <- c(0.7, 0.8, 0.9, 1)
```


```{r}
plot_comparison(trained_mods_glmnet, 6, upper_xlim = 0.8)
glmnet_grid$alpha2[[6]]  <- c(0, 0.1)
```
```{r}
plot_comparison(trained_mods_glmnet, 7)
glmnet_grid$alpha2[[7]]  <- c(0.7, 0.8, 0.9, 1)
```

```{r}
plot_comparison(trained_mods_glmnet, 8, upper_xlim = 0.8)
glmnet_grid$alpha2[[8]]  <- c(0, 0.1)
```

```{r}
summary(glmnet_grid$alpha2)
```

We update the analyses with the new alpha vectors and with a shorter but more exact vector for lambda.

```{r}

start <- Sys.time()

trained_mods_glmnet2 <- 
  purrr::map2(
    .x = glmnet_grid$formula,
    .y = glmnet_grid$alpha2, # updated
    .f = ~ caret::train(
      form      = .x,
      data      = training_set,
      method    = "glmnet",
      family    = "binomial", # passed to glmnet, define as logistic regression
      standardize = TRUE,     # passed to glmnet, explicitly standardize  
      metric    = "ROC",
      trControl = ctrl_fun_training,
      tuneGrid  = expand.grid(alpha  = .y,
                              lambda = seq(0, 0.6, by = 0.005)) #updated
      )
    )
    

time_to_run <- Sys.time() - start
time_to_run

```

```{r message=FALSE, warning=FALSE}
devtools::wd()
saveRDS(trained_mods_glmnet2, "not_public/trained_mods_glmnet2.rds")
```


```{r}
plot_comparison(trained_mods_glmnet2, 1, upper_xlim = 0.05, 
               lower_ylim_range = 0.001)
glmnet_grid$alpha3[[1]] <- c(0.95, 1)
```
```{r}
plot_comparison(trained_mods_glmnet2, 2)
glmnet_grid$alpha3[[2]] <- c(0.05, 0.1, 0.15)
```
```{r}
plot_comparison(trained_mods_glmnet2, 3, upper_xlim = 0.4)
glmnet_grid$alpha3[[3]]  <- c(0.15, 0.2, 0.25)
```

```{r}
plot_comparison(trained_mods_glmnet2, 4, upper_xlim = 0.4, 
               lower_ylim_range = 0.03)
glmnet_grid$alpha3[[4]]  <- c(0, 0.05)
```
```{r}
plot_comparison(trained_mods_glmnet2, 5, upper_xlim = 0.05)
glmnet_grid$alpha3[[5]]  <- c(0.95, 1)
```


```{r}
plot_comparison(trained_mods_glmnet2, 6, upper_xlim = 0.4)
glmnet_grid$alpha3[[6]]  <- c(0.05, 0.1, 0.15)
```
```{r}
plot_comparison(trained_mods_glmnet2, 7, upper_xlim = 0.05)
glmnet_grid$alpha3[[7]]  <- c(0.7, 0.75, 0.8, 0.85, 
                              0.9, 0.95, 1)
```

```{r}
plot_comparison(trained_mods_glmnet2, 8, upper_xlim = 0.4)
glmnet_grid$alpha3[[8]]  <- c(0.05, 0.1, 0.15)
```

```{r}
summary(glmnet_grid$alpha3)
```
```{r}

start <- Sys.time()

trained_mods_glmnet3 <- 
  purrr::map2(
    .x = glmnet_grid$formula,
    .y = glmnet_grid$alpha3, #updated
    .f = ~ caret::train(
      form      = .x,
      data      = training_set,
      method    = "glmnet",
      family    = "binomial", # passed to glmnet, define as logistic regression
      standardize = TRUE,     # passed to glmnet, explicitly standardize  
      metric    = "ROC",
      trControl = ctrl_fun_training,
      tuneGrid  = expand.grid(alpha  = .y,
                              lambda = seq(0, 0.4, by = 0.002)) #updated
      )
    )
    

time_to_run <- Sys.time() - start
time_to_run

```
```{r message=FALSE, warning=FALSE}
devtools::wd()
saveRDS(trained_mods_glmnet3, "not_public/trained_mods_glmnet3.rds")
```

```{r}
plot_comparison(trained_mods_glmnet3, 1, upper_xlim = 0.05, 
               lower_ylim_range = 0.001)
```
```{r}
plot_comparison(trained_mods_glmnet3, 2, lower_ylim_range = 0.001)

```
```{r}
plot_comparison(trained_mods_glmnet3, 3, upper_xlim = 0.4, 
                lower_ylim_range = 0.001)
```

```{r}
plot_comparison(trained_mods_glmnet3, 4, upper_xlim = 0.4, 
               lower_ylim_range = 0.01)
```
```{r}
plot_comparison(trained_mods_glmnet3, 5, upper_xlim = 0.05,
                lower_ylim_range = 0.001)
```


```{r}
plot_comparison(trained_mods_glmnet3, 6, upper_xlim = 0.4, 
                lower_ylim_range = 0.01)

```
```{r}
plot_comparison(trained_mods_glmnet3, 7, upper_xlim = 0.05,
                lower_ylim_range = 0.001)
```

```{r}
plot_comparison(trained_mods_glmnet3, 8, upper_xlim = 0.4,
                lower_ylim_range = 0.001)
```
